# ğŸŒŸ Dark Data Rises: ETL to Snowflake to Visualization

![Dark Data Rises Thumbnail](images/thumbnail/thumbnail_image.png)

A complete data pipeline project that transforms raw data into valuable insights through ETL, data warehousing, and interactive visualizations.

## ğŸ¯ Project Overview

This project demonstrates a full data pipeline:
- ğŸ“¥ Extract data from CSV
- ğŸ”„ Transform and clean data
- ğŸ“¤ Load into Snowflake
- ğŸ“Š Visualize with interactive dashboards

## ğŸ› ï¸ Tech Stack

- **ETL**: Python
- **Data Warehouse**: Snowflake
- **Data Transformation**: dbt
- **Visualization**: Plotly
- **Testing**: dbt Tests
- **Logging**: Python Logging

## ğŸ“ Project Structure

```
â”œâ”€â”€ data/               # Data files
â”‚   â”œâ”€â”€ raw_data.csv
â”‚   â””â”€â”€ cleaned_data.csv
â”œâ”€â”€ etl_python/         # ETL scripts
â”œâ”€â”€ dbt_project/        # dbt models and tests
â”œâ”€â”€ visualization/      # Interactive dashboards
â”œâ”€â”€ sql/               # SQL queries
â””â”€â”€ log_file.log       # Log file
```

## ğŸš€ Features

### 1. ETL Pipeline
- âœ… Data extraction from CSV
- âœ… Data cleaning and transformation
- âœ… Loading to Snowflake
- âœ… Error handling and logging

### 2. Data Quality
- âœ… 27 dbt tests
- âœ… Data validation
- âœ… Logging system
- âœ… Error tracking

### 3. Visualization
- âœ… Interactive Plotly dashboard
- âœ… Product filters
- âœ… Dark theme
- âœ… HTML export

## ğŸ¨ Visualization Features

- ğŸ“Š Bar chart with product quantities
- ğŸ¥§ Pie chart for order status
- ğŸ” Interactive filters
- ğŸ¯ Hover tooltips
- ğŸ’¾ Export to HTML

## ğŸ§ª Testing

- 27 dbt tests in macros folder
- Data quality checks
- Transformation tests
- Clean table tests

## ğŸ“ˆ Data Flow

1. Raw Data â†’ ETL Pipeline
2. ETL Pipeline â†’ Snowflake
3. Snowflake â†’ dbt Models
4. dbt Models â†’ Visualization

## ğŸš€ Getting Started

1. Install dependencies:
```bash
pip install -r requirements.txt
```

2. Run ETL pipeline:
```bash
python etl_pipeline.py
```

3. View visualizations:
```bash
cd visualization
python analyze.py
```

4. Run dbt workflow:
```bash
cd dbt_project
dbt run        # Run all models to ensure data is clean
dbt test       # Run all 27 tests to validate data
dbt docs generate  # Generate documentation
dbt docs serve    # View documentation in browser
```

The complete workflow ensures:
- âœ… Data is properly loaded (ETL)
- âœ… Data is clean and transformed (dbt run)
- âœ… All tests pass (dbt test)
- âœ… Documentation is up to date (dbt docs)

## ğŸ“Š Output

- Cleaned data in Snowflake
- Interactive HTML dashboard
- Static PNG charts
- Log file for tracking
- dbt test results
- dbt documentation

## ğŸ” Monitoring

- Log file in root directory
- dbt test results (27 tests)
- ETL pipeline status
- dbt documentation at http://localhost:8080

## ğŸ¯ Future Improvements

1. Data quality monitoring dashboard
2. Performance optimization
3. Real-time data processing
4. Enhanced documentation
5. Data security features

## ğŸ‘¥ Contributing

Feel free to fork and contribute to this project!

## ğŸ“ License

This project is open source and available under the MIT License.

---

## ğŸ§  Project Overview

This project includes:

- âœ… ETL (Extract, Transform, Load) process using Python  
- â„ï¸ Loading the cleaned data into **Snowflake**  
- ğŸ§® Using **SQL** to extract insights from the data  
- ğŸ“Š Visualizing insights with **matplotlib** and **seaborn**

---

## ğŸ“ Data Folder

- `raw_data.csv` â†’ Contains the original **100,000 rows** of unprocessed data  
- `cleaned_data.csv` â†’ The result of cleaning `raw_data.csv` through ETL process

---

## ğŸ§° ETL Process (in `etl_python` folder)

The ETL is broken into three main steps:

1. **Extract** data from `raw_data.csv`  
2. **Transform** it (cleaning, formatting, etc.)  
3. **Load** the final cleaned data into **Snowflake**

To perform the ETL:

> ğŸš€ Run `etl_pipeline.py` (which is **outside** the `etl_python` folder)  
> This script will trigger the full ETL pipeline and load the data to Snowflake.

---

## ğŸ–¼ï¸ Images Folder

This folder contains screenshots of the project stages:

- Before & after data cleaning  
- Before & after loading to Snowflake  
- Logs from ETL in `log_file.log`  
  *(ğŸ“ Currently empty because ETL hasn't been run yet)*

---

## ğŸ“ SQL Folder

Contains SQL questions and solutions to analyze the data after it's loaded into Snowflake.

- Each SQL file focuses on a specific topic (e.g., filtering, aggregation, window functions, etc.)  
- Run them after ETL to explore insights from your cleaned data âœ…

---

## ğŸ“Š Visualization Folder

Includes `analyze.py` that visualizes data using:

- ğŸ“‰ Bar chart (with `matplotlib`)  
- ğŸ¥§ Pie chart (with `seaborn`)  

**To use:**

1. Run `analyze.py`  
2. You will see interactive dashboard of "bar graph" and "pie chart" 
3. for backup we have .png files seperately ğŸ‰

---

## â–¶ï¸ How to Use This Project

1. ğŸ“¥ First, install the required Python libraries (see Requirements section below)  
2. âš™ï¸ Run `etl_pipeline.py` â†’ This will extract, clean, and load data into **Snowflake**  
3. ğŸ” Open the **SQL folder** and try the SQL questions to analyze the loaded data  
4. ğŸ“Š Run `analyze.py` in the **visualization folder**  
   - It will take u to a window and show you a interactive dashboard  
   - Enjoy bar and pie charts generated from the clean data!  

---

## ğŸ“¦ Requirements

Install the required dependencies with:

```bash
pip install -r requirements.txt